{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc743d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b99675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "basic_stopwords = stopwords.words('english')\n",
    "\n",
    "df = pd.read_csv(\"dataset2.csv\",encoding='latin')\n",
    "df = df.drop(df.columns[[1,2,3,4]],axis=1)\n",
    "df = df.rename(columns={df.columns[0]: 'target', df.columns[1]: 'tweet'})\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower() \n",
    "    tweet = re.sub('@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+',' ', tweet)\n",
    "    tweet = nltk.word_tokenize(tweet)\n",
    "    tweet = [word for word in tweet if word not in basic_stopwords and word.isalnum()]\n",
    "    return tweet\n",
    "\n",
    "df.tweet =df.tweet.apply(lambda x:preprocess(x)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Word2Vec(df.tweet,window=4,workers=4,min_count=1)\n",
    "all_normed_vectors = model.wv.get_normed_vectors()\n",
    "model.save('model.bin')\n",
    "\n",
    "\n",
    "\n",
    "word_vectors = {}\n",
    "\n",
    "file = open('model.txt',encoding='utf-8')\n",
    "\n",
    "for line in file:\n",
    "    vector_values = line.split()\n",
    "    vector = np.asarray(vector_values[1:])\n",
    "    word_vectors[vector_values[0]] = vector\n",
    "    \n",
    "    \n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "tokenizer.fit_on_texts(df.tweet)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df.tweet)\n",
    "padded = pad_sequences(sequences)\n",
    "\n",
    "\n",
    "vocabulary = tokenizer.word_index\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit_transform(df.target.to_list())\n",
    "\n",
    "x_train = padded[:1200000]\n",
    "x_test = padded[1200001:]\n",
    "y_train = df.loc[1:1200000,'target'].values\n",
    "y_test = df.loc[1200001:,'target'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_words = len(vocabulary) + 1\n",
    "\n",
    "embedded_matrix = np.zeros((num_words,100))\n",
    "for word,i in vocabulary.items():\n",
    "    embedded_vector = word_vectors.get(word)\n",
    "    embedded_matrix[i] = embedded_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72ab3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r',encoding=\"utf-8\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix\n",
    "\n",
    "# contains the index for each word\n",
    "vocab = tokenizer.word_index\n",
    "# total number of words in our vocabulary, plus one for unknown words\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# embedding dimensions\n",
    "embedding_dim = 200\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('glove.twitter.27B.200d.txt')\n",
    "# get vectors in the right order\n",
    "embedding_matrix = get_weight_matrix(raw_embedding, vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d2c1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 200)           67100600  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                12864     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 67,434,329\n",
      "Trainable params: 333,729\n",
      "Non-trainable params: 67,100,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "embedding_layer = Embedding(num_words, \n",
    "                            200, \n",
    "                            weights = [embedding_matrix], \n",
    "                            input_length = 50, \n",
    "                            trainable = False)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(200, dropout = 0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51399f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 15\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                              factor = 0.1,\n",
    "                              min_lr = 0.01)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS,\n",
    "                    validation_split = 0.1, verbose = 1, callbacks = [reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11376d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
